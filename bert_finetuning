from transformers import BertForSequenceClassification, BertTokenizer, TrainingArguments, Trainer, DataCollatorWithPadding
from sklearn.model_selection import train_test_split
import numpy as np
import torch
from torch.utils.data import Dataset, TensorDataset, DataLoader
import string

TEST_SIZE = 0.2
SEED = 42
LEARNING_RATE = 2e-5
LOGGING_STEPS = 500
EVAL_STEPS = 500
NUM_TRAIN_EPOCHS = 3
BATCH_SIZE = 8


# Load pre-trained BERT model and tokenizer
model_name = "bert-base-uncased"
model = BertForSequenceClassification.from_pretrained(model_name, num_labels=3)
tokenizer = BertTokenizer.from_pretrained(model_name)

# Assuming 'texts' is a list of news articles and 'labels' is a list of their corresponding labels (0 for unbiased, 1 for biased)
# Preprocess the data

print("loading data")
articles = np.load("data/articles.npy")
labels = np.load("data/labels_1hot.npy")

texts = articles.tolist()

# removes punctuations
texts = [article.translate(None,string.punctuation) for article in texts]

print("tokenizing")
inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="pt")
labels = torch.tensor(labels)

# Split data into train and validation sets
train_inputs, val_inputs, train_labels, val_labels = train_test_split(inputs, labels, test_size=TEST_SIZE, random_state=SEED)

print("DataCollatorWithPadding")
# Define data collator
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

print("TensorDataset")
# Assuming 'inputs' and 'labels' are already tensors
train_dataset = TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], train_labels)
val_dataset = TensorDataset(val_inputs['input_ids'], val_inputs['attention_mask'], val_labels)

print("DataLoader")
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)


# Define training arguments and trainer
training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    logging_dir="./logs",
    logging_steps=500,
    evaluation_strategy="steps",
    eval_steps=EVAL_STEPS,
    save_steps=500,
    num_train_epochs=NUM_TRAIN_EPOCHS,
    learning_rate=LEARNING_RATE,
    logging_first_step=True,
    overwrite_output_dir=True,
)

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
)

print("starting training")
# Fine-tune the model
trainer.train()

# Evaluate the model
results = trainer.evaluate()

# Save the model
model.save_pretrained("./biased_detection_model")
tokenizer.save_pretrained("./biased_detection_model")
